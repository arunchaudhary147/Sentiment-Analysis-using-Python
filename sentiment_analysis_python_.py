# -*- coding: utf-8 -*-
"""Sentiment_Analysis_Python_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/arunchaudhary147/Sentiment-Analysis-using-Python/blob/main/Sentiment_Analysis_Python_.ipynb

# **Project Name - Sentiment Analysis using Python**
One of the applications of text mining is sentiment analysis. Most of the data is getting generated in textual format and in the past few years. Improvement is a continuous process and many product based companies leverage these text mining techniques to examine the sentiments of the customers to find about what they can improve in the product. This information also helps them to understand the trend and demand of the end user which results in Customer satisfaction.

As text mining is a vast concept, the article is divided into two subchapters. The main focus of this article will be calculating two scores: sentiment polarity and subjectivity using python. The range of polarity is from -1 to 1(negative to positive) and will tell us if the text contains positive or negative feedback. Most companies prefer to stop their analysis here but in our second article, we will try to extend our analysis by creating some labels out of these scores.

# **GitHub Link -** https://github.com/arunchaudhary147

## Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import re
import string
import numpy as np
import random
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from wordcloud import WordCloud
from textblob import TextBlob

# Mounting the google drive to access the files
from google.colab import drive
drive.mount('/content/drive')

"""## **Dataset Loading**"""

df = pd.read_csv('/content/drive/MyDrive/Sample Data.csv')

df.head()

"""# Data Preprocessing
Now we will perform various pre-processing steps on the dataset that mainly dealt with removing stopwords, removing emojis. The text document is then converted into the lowercase for better generalization.

Subsequently, the punctuations will be cleaned and removed thereby reducing the unnecessary noise from the dataset. After that, we will also remove the repeating characters from the words along with removing the URLs as they do not have any significant importance.

At last, we will then perform Stemming(reducing the words to their derived stems) and Lemmatization(reducing the derived words to their root form known as lemma) for better results.

## Data Cleaning
"""

df.fillna('', inplace=True)

df.shape

import nltk
from nltk.stem import WordNetLemmatizer
lemma = WordNetLemmatizer()
nltk.download('stopwords')
from nltk.corpus import stopwords

"""#### Making statement text in lower case"""

df['content']=df['content'].str.lower()
df['content'].head()

"""#### Cleaning and removing the above stop words list from the text"""

STOPWORDS = set(stopwords.words('english'))
def cleaning_stopwords(text):
    return " ".join([word for word in str(text).split() if word not in STOPWORDS])
df['content'] = df['content'].apply(lambda text: cleaning_stopwords(text))
df['content'].head()

"""### Removing punctuation, numbers and special characters
#### This will replace everything except characters and hashtags with spaces. "[^a-zA-Z#]" this regular expression means everything except alphabets and hashtags.

#### Cleaning and removing punctuations
"""

import string
english_punctuations = string.punctuation
punctuations_list = english_punctuations
def cleaning_punctuations(text):
    translator = str.maketrans('', '', punctuations_list)
    return text.translate(translator)
df['content']= df['content'].apply(lambda x: cleaning_punctuations(x))
df['content'].head()

"""#### Cleaning and removing repeating characters"""

def cleaning_repeating_char(text):
    return re.sub(r'(.)1+', r'1', text)
df['content'] = df['content'].apply(lambda x: cleaning_repeating_char(x))
df['content'].head()

"""#### Cleaning and removing URL’s"""

def cleaning_URLs(data):
    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)
df['content'] = df['content'].apply(lambda x: cleaning_URLs(x))
df['content'].head()

"""#### Cleaning and removing Numeric numbers"""

def cleaning_numbers(data):
    return re.sub('[0-9]+', ' ', data)
df['content'] = df['content'].apply(lambda x: cleaning_numbers(x))
df['content'].head()

"""### Remove short words
#### We remove those words which are of little or no use. So, we will select the length of words which we want to remove
"""

def transform_text(text):
    return ' '.join([word for word in text.split() if len(word) > 2])
df['content'] = df['content'].apply(lambda x: transform_text(x))
df['content'].head()

"""## Tokenization
Tokenization is a way to split the strings into a list of words. In this example you’ll use the Natural Language Toolkit which has built-in functions for tokenization.
we can also use regex to tokenize it but it is a bit difficult. Though it gives you more control over our text

#### Getting tokenization of tweet text
"""

# Function which directly tokenize the tweet data
from nltk.tokenize import TweetTokenizer

tt = TweetTokenizer()
df['content']=df['content'].apply(tt.tokenize)
df['content'].head()

"""#### Applying Stemming"""

import nltk
st = nltk.PorterStemmer()
def stemming_on_text(data):
    text = [st.stem(word) for word in data]
    return data
df['content']= df['content'].apply(lambda x: stemming_on_text(x))
df['content'].head()

"""#### Applying Lemmatizer"""

import nltk
nltk.download('wordnet')

lm = nltk.WordNetLemmatizer()
def lemmatizer_on_text(data):
    text = [lm.lemmatize(word) for word in data]
    return data
df['content'] = df['content'].apply(lambda x: lemmatizer_on_text(x))
df['content'].head()

"""## Subjectivity and polarity"""

#create a function to get the subjectivity
def getSubjectivity(text):
    # Join the list of words into a single string using a space separator
    text = ' '.join(text)
    return TextBlob(text).sentiment.subjectivity

#create a function to get the polarity
def getpolarity(text):
    # Join the list of words into a single string using a space separator
    text = ' '.join(text)
    return TextBlob(text).sentiment.polarity

#create two new columns
df['subjectivity']=df['content'].apply(getSubjectivity)
df['polarity']=df['content'].apply(getpolarity)

#show the new datafraem with the new columns
df.head()

"""## Compute the negative, neutral and positive analysis"""

#create a function to compute the negative, neutral and positive analysis
def getAnalysis(score):
    if score<0:
        return 'negative'
    elif score==0:
        return 'neutral'
    else:
        return 'positive'

df['analysis']=df['polarity'].apply(getAnalysis)

#show dataFrame
df.head()

# create two new dataframe all of the positive text
df_positive = df[df['analysis'] == 'positive']


# create two new dataframe all of the negative text
df_negative = df[df['analysis'] == 'negative']


# create two new dataframe all of the neutral text
df_neutral=df[df['analysis'] == 'neutral']

"""#### Count the number of positive, negative, neutral reviews."""

tb_counts = df.analysis.value_counts()
tb_counts

"""# Data Exploration
## Let's form a WordCloud
### A wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes
"""

# Visualizing all tweets

all_words = " ".join(" ".join(sent) for sent in df['content'])

from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)

plt.figure(figsize=(15,8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""## Positive tweets"""

# Visualizing all positive tweets

all_pos_words = " ".join(" ".join(sent) for sent in df_positive['content'])

from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_pos_words)

plt.figure(figsize=(15,8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""## Negative tweets"""

# Visualizing all negative tweets

all_neg_words = " ".join(" ".join(sent) for sent in df_negative['content'])

from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_neg_words)

plt.figure(figsize=(15,8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""## Neutral tweets"""

# Visualizing all neutral tweets

all_neu_words = " ".join(" ".join(sent) for sent in df_neutral['content'])

from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_neu_words)

plt.figure(figsize=(15,8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

#plot the polarity and subjectivity
plt.figure(figsize=(8,6))
plt.scatter(df['polarity'], df['subjectivity'], color='blue')
plt.title('Sentiment Analysis')
plt.xlabel('Polarity')
plt.ylabel('Subjectivity')
plt.show()

# Get the percentage of positive tweets
print("Positive tweets",round((df_positive.shape[0]/df.shape[0])*100,1),"%")
# Get the percentage of negative tweets
print("Negative tweets",round((df_negative.shape[0]/df.shape[0])*100,1),"%")
# Get the percentage of neutral tweets
print("Neutral tweets",round((df_neutral.shape[0]/df.shape[0])*100,1),"%")

# show the value counts

df['analysis'].value_counts()

#plot and visualize the counts
plt.title('Sentiment Analysis')
plt.xlabel('Sentiment')
plt.ylabel('Count')
df['analysis'].value_counts().plot(kind='bar')
plt.show()

"""# **Conclusion**
We can see that maximum percentage of neutral tweets 47.8% , minimum percentage of negative tweets 7.5% and average percentage of positive tweets 44.7%.
"""